{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Problem Set 3\n",
    "# Due Wednesday, November 13, 2019\n",
    "\n",
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "In this problem set, you are to implement a two-layer (2 layers of weights, 1 hidden layer of units) neural network for binary classification.  All non-linearities are to be sigmoids.\n",
    "\n",
    "Details are given below.  *Please read the **entire** notebook carefully before proceeding.*\n",
    "\n",
    "You need to both fill in the necesary code, **and** answer the question at the bottom.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the only imports that are necessary (or allowed)\n",
    "import numpy as np\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "We will be using a USPS digit dataset (provided in the file uspsall73.mat).\n",
    "It has 16-by-16 grayscale images of each of the 10 different hand-written digits\n",
    "However, we will load only two of the digits to use as the two classes in\n",
    "binary classification\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load two of the 10 classes (c1 is for Y=+1 and c2 is for Y=0)\n",
    "# Note that for neural networks, we will be using Y={+1,0} instead of Y={+1,-1}\n",
    "def loaddigitdata(c1,c2,m):\n",
    "    f = h5py.File('uspsall73.mat','r') \n",
    "    data = f.get('data') \n",
    "    data = np.array(data).astype(float)\n",
    "    X = np.concatenate((data[c1,:,:],data[c2,:,:]))\n",
    "    Y = np.concatenate((np.zeros((data.shape[1])),np.ones((data.shape[1]))))\n",
    "    \n",
    "    rndstate = np.random.get_state() # going to set the \"random\" shuffle random seed\n",
    "    np.random.seed(132857) # setting seed so that dataset is consistent\n",
    "    p = np.random.permutation(X.shape[0])\n",
    "    X = X[p] # this and next line make copies, but that's okay given how small our dataset is\n",
    "    Y = Y[p]\n",
    "    np.random.set_state(rndstate) # reset seed\n",
    "    \n",
    "    trainX = X[0:m,:] # use the first m (after shuffling) for training\n",
    "    trainY = Y[0:m,np.newaxis]\n",
    "    validX = X[m:,:] # use the rest for validation\n",
    "    validY = Y[m:,np.newaxis]\n",
    "    return (trainX,trainY,validX,validY)\n",
    "\n",
    "# In case you care (not necessary for the assignment)\n",
    "def drawexample(x,ax=None): # takes an x *vector* and draws the image it encodes\n",
    "    if ax is None:\n",
    "        plt.imshow(np.reshape(x,(16,16)).T,cmap='gray')\n",
    "    else:\n",
    "        ax.imshow(np.reshape(x,(16,16)).T,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data, to differentiate between 7s and 9s\n",
    "# we will use on 110 examples for training (50% of the data) and the other half for validation\n",
    "(trainX,trainY,validX,validY) = loaddigitdata(6,8,1100)\n",
    "means = trainX.mean(axis=0)\n",
    "stddevs = trainX.std(axis=0)\n",
    "stddevs[stddevs<1e-6] = 1.0\n",
    "trainX = (trainX-means)/stddevs\n",
    "validX = (validX-means)/stddevs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert this cell to a code cell if you wish to see each of the examples, plotted\n",
    "# (completely not necessary for the problem set)\n",
    "f = plt.figure()\n",
    "f.set_size_inches(8,8)\n",
    "\n",
    "ax = f.add_subplot(111)\n",
    "plt.ion()\n",
    "f.canvas.draw()\n",
    "for exi in range(trainX.shape[0]):\n",
    "    display.clear_output(wait=True)\n",
    "    drawexample(trainX[exi,:])\n",
    "    digitid = (9 if trainY[exi]>0 else 7)\n",
    "    ax.set_title('y = '+str(int(trainY[exi]))+\" [\"+str(digitid)+\"]\")\n",
    "    display.display(f)\n",
    "    #time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITE `nneval` and `trainneuralnet` [20 points]\n",
    "\n",
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "This is the main portion of the assignment\n",
    "\n",
    "Note that the $Y$ values are +1 and 0 (not +1 and -1).  This is as in class and works better with a sigmoid output.\n",
    "\n",
    "You need to write the two functions below (plus any more you would like to add to help): `nneval` and `trainneuralnet`.  The first takes an array/matrix of X vectors and the weights from a neural network and returns a vector of predicted Y values (should be numbers between 0 and 1 -- the probability of class +1, for each of the examples).  The second takes a data set (Xs and Ys), the number of hidden units, and the lambda value (for regularization), and returns the weights.  W1 are the weights from the input to the hidden and W2 are the weights from the hidden to the output.\n",
    "\n",
    "A few notes:\n",
    "- **Starting Weights**: The code supplied randomly selects the weights near zero.  https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79 has a reasonable explanation of why we are doing it this way.  But for the purposes of the assignment, you can just accept this is a good way to initialize neural network weights.\n",
    "- **Offset Terms**: Each layer should have an \"offset\" or \"intercept\" unit (to supply a 1 to the next layer), except the output layer.\n",
    "- **Batch Updates**: For a problem this small, use batch updates.  That is, the step is based on the sum of the gradients for each data point in the training set.\n",
    "- **Step Size**: http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms describes a number of methods to adaptively control $\\eta$ for fast convergence.  You don't need to understand any of them; however, without them, convergence to good solutions on this problem can be quite slow.  Therefore, *use RMSprop*: the code below has a simple version of RMSprop that is sufficient for this assignment.  You need to supply the code that calculates `sumofgrad2` which should be the sum of the square of each element of the gradient (the squared length of the gradient).  (for debugging, feel free to use a constant $\\eta$). \n",
    "- **Stopping Criterion**: To determine when to stop, check the loss function every 10 iterations.  If it has not improved by at least $10^{-8}$ over those 10 iterations, stop.\n",
    "- **Regularization**: You should penalize (from the regularization) all of the weights, even those coming out of offset units.  While it makes sense sometimes not to penalize the ones for the constant $1$ units, you'll find this easier if you just penalize them all.\n",
    "\n",
    "Tips that might help:\n",
    "- Display the loss function's value every 10 iterations (or so).  It should generally be getting smaller.\n",
    "- The smaller $\\lambda$ is and the more units, the more difficult (long) the optimization will be.\n",
    "- Write a function to do forward propagation and one to do backward propagation.  Write a function to evaluate the loss function.  In general, break things up to keep things straight.\n",
    "- Processing the entire batch at once is more efficient in numpy.  Use numpy broadcasting to avoid loops where possible.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FEEL FREE TO ADD HELPER FUNCTIONS\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def loss(f, Y):\n",
    "    if f == 0:\n",
    "        return -((1 - Y) * np.log(1 - f))\n",
    "    return -(Y * np.log(f) + (1 - Y) * np.log(1 - f))\n",
    "\n",
    "def addones(z):\n",
    "    if len(z.shape) == 1:\n",
    "        return np.concatenate((np.ones(1), z))\n",
    "    else:\n",
    "        m = z.shape[0]\n",
    "        return np.hstack((np.ones((m,1)), z))\n",
    "    \n",
    "def gprime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def nneval(X,W1,W2):\n",
    "    # W1 and W2 are as from trainneuralnet\n",
    "    # X is number of examples by number of features\n",
    "    # this should return a vector that has length of the number of examples\n",
    "    # the values in the return vector should be the predicted probabilities of class +1\n",
    "    (m,n) = X.shape\n",
    "    X = addones(X)\n",
    "    Y = np.zeros(m)\n",
    "    i = 0\n",
    "    for i in range(m):\n",
    "        Z1 = W1@X[i]\n",
    "        A = addones(sigmoid(Z1))\n",
    "        Z2 = W2@A\n",
    "        Y[i] = sigmoid(Z2)\n",
    "    return Y\n",
    "  \n",
    "def trainneuralnet(X,Y,nhid,lam):\n",
    "    (m,n) = X.shape\n",
    "    W1 = (np.random.rand(nhid,n+1)*2-1)*np.sqrt(6.0/(n+nhid+1)) # weights to each hidden unit from the inputs (plus the added offset unit)\n",
    "    W2 = (np.random.rand(1,nhid+1)*2-1)*np.sqrt(6.0/(nhid+2)) # weights to the single output unit from the hidden units (plus the offset unit)\n",
    "    W1[:,0] = 0\n",
    "    W2[:,0] = -nhid/2.0\n",
    "    (W1_m,W1_n)= W1.shape\n",
    "    Eg2=1\n",
    "    check = 0\n",
    "    cnt = 0\n",
    "    deltaF = np.zeros(m)\n",
    "    deltaH = np.zeros((m, nhid))\n",
    "    gradF = np.zeros((m, nhid+1))\n",
    "    gradH = np.zeros((m, W1_m, W1_n))\n",
    "    totH = W1\n",
    "    totH = totH - W1\n",
    "    totF = W2\n",
    "    totF = totF - W2\n",
    "    saveL = 0\n",
    "    totL = 0\n",
    "    while 1:\n",
    "        for t in range(m):\n",
    "            Z1 = W1 @ addones(X[t])\n",
    "            A = addones(sigmoid(Z1))\n",
    "            Z2 = W2 @ A\n",
    "            f = sigmoid(Z2)\n",
    "            deltaF[t] = f - Y[t]\n",
    "            deltaH[t] = gprime(Z1) * (W2[0,1:].T * deltaF[t])\n",
    "            gradF[t] = deltaF[t] * A    \n",
    "            gradH[t] = deltaH[t][:, None] @ addones(X[t])[None,:]\n",
    "            eta = 0.001\n",
    "            totH = totH + (gradH[t]) + (2 * lam * W1 / m)\n",
    "            totF = totF + (gradF[t]) + (2 * lam * W2 / m)\n",
    "            totL = totL + loss(f, Y[t])\n",
    "        sumofgrad2 = (gradH * gradH).sum() + (gradF * gradF).sum()#??? sum of the squares of all of the elements of the gradient\n",
    "        Eg2 = 0.9*Eg2 + 0.1*sumofgrad2\n",
    "        eta = 0.1/(np.sqrt((1e-10+Eg2)))\n",
    "        W1 = W1 - (eta * totH)\n",
    "        W2 = W2 - (eta * totF)\n",
    "        cnt = cnt + 1\n",
    "        L = (totL / m) + (lam * ((W1 * W1).sum() + (W2 * W2).sum()) / m)\n",
    "        if cnt == 10:\n",
    "            cnt = 0\n",
    "            print(saveL)\n",
    "            print(np.fabs(saveL - L))\n",
    "            if np.fabs(saveL - L) < 1e-8:\n",
    "                break\n",
    "            saveL = L\n",
    "        totL = 0\n",
    "        totH = 0\n",
    "        totF = 0\n",
    "    # use eta here to make update\n",
    "    return (W1,W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.11214475]\n",
      "[0.11214475]\n",
      "[0.03159817]\n",
      "[0.08054658]\n",
      "[0.00997002]\n",
      "[0.07057656]\n",
      "[0.00519485]\n",
      "[0.06538171]\n",
      "[0.00363558]\n",
      "[0.06174613]\n",
      "[0.00284483]\n",
      "[0.0589013]\n",
      "[0.00182262]\n",
      "[0.05707867]\n",
      "[0.00139869]\n",
      "[0.05567999]\n",
      "[0.00111557]\n",
      "[0.05456442]\n",
      "[0.00090087]\n",
      "[0.05366355]\n",
      "[0.0007256]\n",
      "[0.05293794]\n",
      "[0.00058285]\n",
      "[0.05235509]\n",
      "[0.00046882]\n",
      "[0.05188627]\n",
      "[0.00037908]\n",
      "[0.0515072]\n",
      "[0.00031082]\n",
      "[0.05119638]\n",
      "[0.00026356]\n",
      "[0.05093281]\n",
      "[0.00023723]\n",
      "[0.05069558]\n",
      "[0.00023209]\n",
      "[0.05046349]\n",
      "[0.00025302]\n",
      "[0.05021047]\n",
      "[0.00024908]\n",
      "[0.04996139]\n",
      "[0.0001642]\n",
      "[0.04979719]\n",
      "[0.00011523]\n",
      "[0.04968197]\n",
      "[8.69114648e-05]\n",
      "[0.04959505]\n",
      "[6.89365551e-05]\n",
      "[0.04952612]\n",
      "[5.65524452e-05]\n",
      "[0.04946957]\n",
      "[4.74489361e-05]\n",
      "[0.04942212]\n",
      "[4.04389665e-05]\n",
      "[0.04938168]\n",
      "[3.48652809e-05]\n",
      "[0.04934681]\n",
      "[3.03343821e-05]\n",
      "[0.04931648]\n",
      "[2.6592628e-05]\n",
      "[0.04928989]\n",
      "[2.3465116e-05]\n",
      "[0.04926642]\n",
      "[2.08237794e-05]\n",
      "[0.0492456]\n",
      "[1.85700077e-05]\n",
      "[0.04922703]\n",
      "[1.66249582e-05]\n",
      "[0.0492104]\n",
      "[1.49242523e-05]\n",
      "[0.04919548]\n",
      "[1.34153713e-05]\n",
      "[0.04918206]\n",
      "[1.20566798e-05]\n",
      "[0.04917001]\n",
      "[1.08171176e-05]\n",
      "[0.04915919]\n",
      "[9.67568214e-06]\n",
      "[0.04914951]\n",
      "[8.62012363e-06]\n",
      "[0.04914089]\n",
      "[7.6447968e-06]\n",
      "[0.04913325]\n",
      "[6.74808028e-06]\n",
      "[0.0491265]\n",
      "[5.92996035e-06]\n",
      "[0.04912057]\n",
      "[5.19023751e-06]\n",
      "[0.04911538]\n",
      "[4.52751978e-06]\n",
      "[0.04911085]\n",
      "[3.93890514e-06]\n",
      "[0.04910691]\n",
      "[3.42012611e-06]\n",
      "[0.04910349]\n",
      "[2.96592227e-06]\n",
      "[0.04910053]\n",
      "[2.57046672e-06]\n",
      "[0.04909796]\n",
      "[2.22774609e-06]\n",
      "[0.04909573]\n",
      "[1.93185292e-06]\n",
      "[0.0490938]\n",
      "[1.67718563e-06]\n",
      "[0.04909212]\n",
      "[1.45856923e-06]\n",
      "[0.04909066]\n",
      "[1.27131529e-06]\n",
      "[0.04908939]\n",
      "[1.11123907e-06]\n",
      "[0.04908828]\n",
      "[9.74648488e-07]\n",
      "[0.0490873]\n",
      "[8.58316003e-07]\n",
      "[0.04908645]\n",
      "[7.59441093e-07]\n",
      "[0.04908569]\n",
      "[6.75608835e-07]\n",
      "[0.04908501]\n",
      "[6.04747958e-07]\n",
      "[0.04908441]\n",
      "[5.4509059e-07]\n",
      "[0.04908386]\n",
      "[4.95134968e-07]\n",
      "[0.04908337]\n",
      "[4.536118e-07]\n",
      "[0.04908291]\n",
      "[4.19454539e-07]\n",
      "[0.04908249]\n",
      "[3.91773628e-07]\n",
      "[0.0490821]\n",
      "[3.69834603e-07]\n",
      "[0.04908173]\n",
      "[3.53039883e-07]\n",
      "[0.04908138]\n",
      "[3.40914071e-07]\n",
      "[0.04908104]\n",
      "[3.33092575e-07]\n",
      "[0.0490807]\n",
      "[3.29313433e-07]\n",
      "[0.04908037]\n",
      "[3.29412185e-07]\n",
      "[0.04908005]\n",
      "[3.33319725e-07]\n",
      "[0.04907971]\n",
      "[3.4106293e-07]\n",
      "[0.04907937]\n",
      "[3.52767769e-07]\n",
      "[0.04907902]\n",
      "[3.68664222e-07]\n",
      "[0.04907865]\n",
      "[3.89091638e-07]\n",
      "[0.04907826]\n",
      "[4.14502007e-07]\n",
      "[0.04907785]\n",
      "[4.45456519e-07]\n",
      "[0.0490774]\n",
      "[4.82607612e-07]\n",
      "[0.04907692]\n",
      "[5.26653979e-07]\n",
      "[0.04907639]\n",
      "[5.78249958e-07]\n",
      "[0.04907581]\n",
      "[6.37844601e-07]\n",
      "[0.04907517]\n",
      "[7.05423725e-07]\n",
      "[0.04907447]\n",
      "[7.80139015e-07]\n",
      "[0.04907369]\n",
      "[8.5984604e-07]\n",
      "[0.04907283]\n",
      "[9.40651901e-07]\n",
      "[0.04907189]\n",
      "[1.01668809e-06]\n",
      "[0.04907087]\n",
      "[1.08041818e-06]\n",
      "[0.04906979]\n",
      "[1.12373894e-06]\n",
      "[0.04906867]\n",
      "[1.13981804e-06]\n",
      "[0.04906753]\n",
      "[1.1250978e-06]\n",
      "[0.0490664]\n",
      "[1.08054705e-06]\n",
      "[0.04906532]\n",
      "[1.01147254e-06]\n",
      "[0.04906431]\n",
      "[9.25964727e-07]\n",
      "[0.04906339]\n",
      "[8.32766033e-07]\n",
      "[0.04906255]\n",
      "[7.39477846e-07]\n",
      "[0.04906181]\n",
      "[6.51606563e-07]\n",
      "[0.04906116]\n",
      "[5.72428464e-07]\n",
      "[0.04906059]\n",
      "[5.03366245e-07]\n",
      "[0.04906009]\n",
      "[4.4454916e-07]\n",
      "[0.04905964]\n",
      "[3.95341203e-07]\n",
      "[0.04905925]\n",
      "[3.54744641e-07]\n",
      "[0.04905889]\n",
      "[3.21667131e-07]\n",
      "[0.04905857]\n",
      "[2.9507764e-07]\n",
      "[0.04905827]\n",
      "[2.740853e-07]\n",
      "[0.049058]\n",
      "[2.57971222e-07]\n",
      "[0.04905774]\n",
      "[2.46195441e-07]\n",
      "[0.0490575]\n",
      "[2.38393859e-07]\n",
      "[0.04905726]\n",
      "[2.3437477e-07]\n",
      "[0.04905702]\n",
      "[2.34121337e-07]\n",
      "[0.04905679]\n",
      "[2.37804992e-07]\n",
      "[0.04905655]\n",
      "[2.45814892e-07]\n",
      "[0.04905631]\n",
      "[2.58810276e-07]\n",
      "[0.04905605]\n",
      "[2.77806067e-07]\n",
      "[0.04905577]\n",
      "[3.04308208e-07]\n",
      "[0.04905546]\n",
      "[3.40525388e-07]\n",
      "[0.04905512]\n",
      "[3.89700383e-07]\n",
      "[0.04905473]\n",
      "[4.56630593e-07]\n",
      "[0.04905428]\n",
      "[5.48487061e-07]\n",
      "[0.04905373]\n",
      "[6.7609267e-07]\n",
      "[0.04905305]\n",
      "[8.5585616e-07]\n",
      "[0.0490522]\n",
      "[1.11245808e-06]\n",
      "[0.04905108]\n",
      "[1.4817347e-06]\n",
      "[0.0490496]\n",
      "[2.01084798e-06]\n",
      "[0.04904759]\n",
      "[2.7464519e-06]\n",
      "[0.04904485]\n",
      "[3.69062902e-06]\n",
      "[0.04904115]\n",
      "[4.70742773e-06]\n",
      "[0.04903645]\n",
      "[5.44391467e-06]\n",
      "[0.049031]\n",
      "[5.4854158e-06]\n",
      "[0.04902552]\n",
      "[4.78074466e-06]\n",
      "[0.04902074]\n",
      "[3.73713966e-06]\n",
      "[0.049017]\n",
      "[2.76930079e-06]\n",
      "[0.04901423]\n",
      "[2.02777777e-06]\n",
      "[0.0490122]\n",
      "[1.49625263e-06]\n",
      "[0.04901071]\n",
      "[1.11997566e-06]\n",
      "[0.04900959]\n",
      "[8.52193819e-07]\n",
      "[0.04900873]\n",
      "[6.59900921e-07]\n",
      "[0.04900807]\n",
      "[5.20570244e-07]\n",
      "[0.04900755]\n",
      "[4.18769043e-07]\n",
      "[0.04900714]\n",
      "[3.43824969e-07]\n",
      "[0.04900679]\n",
      "[2.88296863e-07]\n",
      "[0.0490065]\n",
      "[2.46960244e-07]\n",
      "[0.04900626]\n",
      "[2.16123137e-07]\n",
      "[0.04900604]\n",
      "[1.93160039e-07]\n",
      "[0.04900585]\n",
      "[1.76192664e-07]\n",
      "[0.04900567]\n",
      "[1.6387036e-07]\n",
      "[0.04900551]\n",
      "[1.5521845e-07]\n",
      "[0.04900535]\n",
      "[1.49532993e-07]\n",
      "[0.0490052]\n",
      "[1.46307381e-07]\n",
      "[0.04900506]\n",
      "[1.45180857e-07]\n",
      "[0.04900491]\n",
      "[1.45902223e-07]\n",
      "[0.04900476]\n",
      "[1.48304113e-07]\n",
      "[0.04900462]\n",
      "[1.5228467e-07]\n",
      "[0.04900446]\n",
      "[1.57794372e-07]\n",
      "[0.04900431]\n",
      "[1.64826432e-07]\n",
      "[0.04900414]\n",
      "[1.73409561e-07]\n",
      "[0.04900397]\n",
      "[1.83602164e-07]\n",
      "[0.04900378]\n",
      "[1.9548716e-07]\n",
      "[0.04900359]\n",
      "[2.09166644e-07]\n",
      "[0.04900338]\n",
      "[2.2475562e-07]\n",
      "[0.04900316]\n",
      "[2.42373929e-07]\n",
      "[0.04900291]\n",
      "[2.621354e-07]\n",
      "[0.04900265]\n",
      "[2.84133173e-07]\n",
      "[0.04900237]\n",
      "[3.08420103e-07]\n",
      "[0.04900206]\n",
      "[3.34983339e-07]\n",
      "[0.04900172]\n",
      "[3.63712611e-07]\n",
      "[0.04900136]\n",
      "[3.94362768e-07]\n",
      "[0.04900097]\n",
      "[4.26512716e-07]\n",
      "[0.04900054]\n",
      "[4.59525402e-07]\n",
      "[0.04900008]\n",
      "[4.92516526e-07]\n",
      "[0.04899959]\n",
      "[5.24342863e-07]\n",
      "[0.04899906]\n",
      "[5.5362313e-07]\n",
      "[0.04899851]\n",
      "[5.78803459e-07]\n",
      "[0.04899793]\n",
      "[5.98274034e-07]\n",
      "[0.04899733]\n",
      "[6.10532252e-07]\n",
      "[0.04899672]\n",
      "[6.14372666e-07]\n",
      "[0.04899611]\n",
      "[6.09069362e-07]\n",
      "[0.0489955]\n",
      "[5.94509227e-07]\n",
      "[0.0489949]\n",
      "[5.71240173e-07]\n",
      "[0.04899433]\n",
      "[5.40417571e-07]\n",
      "[0.04899379]\n",
      "[5.03658657e-07]\n",
      "[0.04899329]\n",
      "[4.62838385e-07]\n",
      "[0.04899283]\n",
      "[4.19871923e-07]\n",
      "[0.04899241]\n",
      "[3.76525674e-07]\n",
      "[0.04899203]\n",
      "[3.34284171e-07]\n",
      "[0.04899169]\n",
      "[2.94281641e-07]\n",
      "[0.0489914]\n",
      "[2.57291609e-07]\n",
      "[0.04899114]\n",
      "[2.2375887e-07]\n",
      "[0.04899092]\n",
      "[1.93855927e-07]\n",
      "[0.04899073]\n",
      "[1.67548288e-07]\n",
      "[0.04899056]\n",
      "[1.44657512e-07]\n",
      "[0.04899041]\n",
      "[1.24915582e-07]\n",
      "[0.04899029]\n",
      "[1.08007971e-07]\n",
      "[0.04899018]\n",
      "[9.36053135e-08]\n",
      "[0.04899009]\n",
      "[8.138504e-08]\n",
      "[0.04899001]\n",
      "[7.10449178e-08]\n",
      "[0.04898993]\n",
      "[6.23104941e-08]\n",
      "[0.04898987]\n",
      "[5.49382245e-08]\n",
      "[0.04898982]\n",
      "[4.87157275e-08]\n",
      "[0.04898977]\n",
      "[4.34602645e-08]\n",
      "[0.04898972]\n",
      "[3.90162319e-08]\n",
      "[0.04898969]\n",
      "[3.52522116e-08]\n",
      "[0.04898965]\n",
      "[3.20579252e-08]\n",
      "[0.04898962]\n",
      "[2.93413123e-08]\n",
      "[0.04898959]\n",
      "[2.70258477e-08]\n",
      "[0.04898956]\n",
      "[2.50481546e-08]\n",
      "[0.04898954]\n",
      "[2.33559287e-08]\n",
      "[0.04898951]\n",
      "[2.19061627e-08]\n",
      "[0.04898949]\n",
      "[2.06636488e-08]\n",
      "[0.04898947]\n",
      "[1.9599729e-08]\n",
      "[0.04898945]\n",
      "[1.86912608e-08]\n",
      "[0.04898943]\n",
      "[1.79197649e-08]\n",
      "[0.04898941]\n",
      "[1.72707222e-08]\n",
      "[0.0489894]\n",
      "[1.67329861e-08]\n",
      "[0.04898938]\n",
      "[1.62982722e-08]\n",
      "[0.04898936]\n",
      "[1.59606848e-08]\n",
      "[0.04898935]\n",
      "[1.57162219e-08]\n",
      "[0.04898933]\n",
      "[1.55621883e-08]\n",
      "[0.04898932]\n",
      "[1.54964081e-08]\n",
      "[0.0489893]\n",
      "[1.5516083e-08]\n",
      "[0.04898929]\n",
      "[1.56160641e-08]\n",
      "[0.04898927]\n",
      "[1.57861901e-08]\n",
      "[0.04898925]\n",
      "[1.60071667e-08]\n",
      "[0.04898924]\n",
      "[1.62441978e-08]\n",
      "[0.04898922]\n",
      "[1.64371754e-08]\n",
      "[0.04898921]\n",
      "[1.64856364e-08]\n",
      "[0.04898919]\n",
      "[1.6225795e-08]\n",
      "[0.04898917]\n",
      "[1.53956299e-08]\n",
      "[0.04898916]\n",
      "[1.3582037e-08]\n",
      "[0.04898914]\n",
      "[1.01412108e-08]\n",
      "[0.04898913]\n",
      "[4.07933221e-09]\n",
      "[[ 0.03950612 -0.09024943 -0.0786402  ... -0.05090829 -0.0198122\n",
      "   0.0398464 ]\n",
      " [-0.03947586  0.10464868  0.04927152 ...  0.08355917 -0.009022\n",
      "  -0.09838726]\n",
      " [-0.01768704  0.08863697  0.05169667 ...  0.07025483  0.00419644\n",
      "  -0.0900081 ]\n",
      " [ 0.03108413 -0.09116027 -0.04728785 ... -0.07138429  0.0063134\n",
      "   0.08212439]\n",
      " [ 0.06922161 -0.08308476 -0.0595227  ... -0.04596557  0.01235085\n",
      "   0.07836308]]\n",
      "[[-0.37244566  1.902198   -2.66334571 -2.43204187  2.14790083  1.85282643]]\n"
     ]
    }
   ],
   "source": [
    "# Use this cell (or others you add) to check your network\n",
    "# I would debug on simple examples you create yourself (trying to understand what happens with\n",
    "#  the full 256-dimensional data is hard)\n",
    "   \n",
    "(W1,W2) = trainneuralnet(trainX,trainY,5,1) #an example of training on the USPS data with 5 hidden units and lambda=1\n",
    "print(W1)\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance plot\n",
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "The code below will plot your algorithm's error rate on this data set for various regularization strengths and numbers of hidden units.\n",
    "\n",
    "Make sure your code works for this plot.\n",
    "\n",
    "My code runs in about 5 minutes (to produce the full plot below)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solutions from PS 2's logistic regression (modified to work with Y=0/1)\n",
    "# for comparison... see plot below\n",
    "def learnlogreg(X,Y,lam):    \n",
    "    (m,n) = X.shape\n",
    "    w = np.zeros((n+1))\n",
    "    \n",
    "    eta = 0.05\n",
    "    \n",
    "    Y11 = Y.copy()\n",
    "    Y11[Y11<0.5] = -1\n",
    "    XY = Y11*np.hstack((np.ones((m,1)),X))\n",
    "    neggrad = np.ones(n)\n",
    "    while(neggrad.dot(neggrad)>1e-5):\n",
    "        predy = sigmoid(XY@w)[:,np.newaxis]\n",
    "        neggrad = ((1.0-predy)*XY).mean(axis=0)\n",
    "        gradreg = (lam/m)*w*2.0\n",
    "        gradreg[0] = 0\n",
    "        neggrad = neggrad-gradreg\n",
    "        \n",
    "        llh = -np.sum(np.log(predy))/m + lam*(np.sum(w*w)-w[0]*w[0])/m\n",
    "        \n",
    "        w = w + eta*neggrad\n",
    "    \n",
    "    return w\n",
    "    \n",
    "def predictlogreg(X,w):\n",
    "    return sigmoid(np.hstack((np.ones((X.shape[0],1)),X))@w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.0905018]\n",
      "[0.0905018]\n",
      "[0.0351028]\n",
      "[0.05539901]\n",
      "[0.0141844]\n",
      "[0.0412146]\n",
      "[0.00704944]\n",
      "[0.03416517]\n",
      "[0.00414276]\n",
      "[0.03002241]\n",
      "[0.00264872]\n",
      "[0.02737369]\n",
      "[0.00181633]\n",
      "[0.02555736]\n",
      "[0.00147414]\n",
      "[0.02408322]\n",
      "[0.00327281]\n",
      "[0.02081042]\n",
      "[0.00190256]\n",
      "[0.01890785]\n",
      "[0.0008321]\n",
      "[0.01807575]\n",
      "[0.0004473]\n",
      "[0.01762846]\n",
      "[0.00030058]\n",
      "[0.01732787]\n",
      "[0.00021966]\n",
      "[0.01710821]\n",
      "[0.00017017]\n",
      "[0.01693804]\n",
      "[0.00013826]\n",
      "[0.01679978]\n",
      "[0.00011633]\n",
      "[0.01668344]\n",
      "[0.00010083]\n",
      "[0.01658262]\n",
      "[9.6396777e-05]\n",
      "[0.01648622]\n",
      "[0.00118159]\n",
      "[0.01530463]\n",
      "[0.00016719]\n",
      "[0.01513743]\n",
      "[3.03347945e-05]\n",
      "[0.0151071]\n",
      "[0.00013732]\n",
      "[0.01496978]\n",
      "[3.23385304e-05]\n",
      "[0.01500212]\n",
      "[1.78433673e-05]\n",
      "[0.01498427]\n",
      "[0.00021354]\n",
      "[0.01477074]\n",
      "[9.60215317e-05]\n",
      "[0.01486676]\n",
      "[7.48224275e-05]\n",
      "[0.01479194]\n",
      "[6.45805841e-05]\n",
      "[0.01472736]\n",
      "[3.27664603e-05]\n",
      "[0.01469459]\n",
      "[2.52375967e-05]\n",
      "[0.01466935]\n",
      "[2.0559109e-05]\n",
      "[0.01464879]\n",
      "[1.70642835e-05]\n",
      "[0.01463173]\n",
      "[1.44058249e-05]\n",
      "[0.01461732]\n",
      "[1.23518161e-05]\n",
      "[0.01460497]\n",
      "[1.07746975e-05]\n",
      "[0.0145942]\n",
      "[9.65058129e-06]\n",
      "[0.01458455]\n",
      "[9.15798214e-06]\n",
      "[0.01457539]\n",
      "[1.041316e-05]\n",
      "[0.01456497]\n",
      "[2.17856409e-05]\n",
      "[0.01454319]\n",
      "[0.00113931]\n",
      "[0.0156825]\n",
      "[0.00134046]\n",
      "[0.01434204]\n",
      "[5.91064614e-05]\n",
      "[0.01428293]\n",
      "[3.74502235e-05]\n",
      "[0.01424548]\n",
      "[2.62072987e-05]\n",
      "[0.01421927]\n",
      "[1.95586421e-05]\n",
      "[0.01419972]\n",
      "[1.5948308e-05]\n",
      "[0.01418377]\n",
      "[1.4089985e-05]\n",
      "[0.01416968]\n",
      "[1.33218022e-05]\n",
      "[0.01415636]\n",
      "[1.3345873e-05]\n",
      "[0.01414301]\n",
      "[1.38815859e-05]\n",
      "[0.01412913]\n",
      "[1.44588489e-05]\n",
      "[0.01411467]\n",
      "[1.44325096e-05]\n",
      "[0.01410024]\n",
      "[0.00123197]\n",
      "[0.01533221]\n",
      "[0.00115679]\n",
      "[0.01417542]\n",
      "[6.34653975e-05]\n",
      "[0.01411195]\n",
      "[2.59344249e-05]\n",
      "[0.01408602]\n",
      "[1.57178951e-05]\n",
      "[0.0140703]\n",
      "[1.17125276e-05]\n",
      "[0.01405859]\n",
      "[9.48465186e-06]\n",
      "[0.0140491]\n",
      "[7.90024859e-06]\n",
      "[0.0140412]\n",
      "[6.67006627e-06]\n",
      "[0.01403453]\n",
      "[5.68455752e-06]\n",
      "[0.01402885]\n",
      "[4.88038818e-06]\n",
      "[0.01402397]\n",
      "[1.84288186e-06]\n",
      "[0.01402581]\n",
      "[0.00021645]\n",
      "[0.01424225]\n",
      "[0.00017805]\n",
      "[0.0140642]\n",
      "[3.22827289e-05]\n",
      "[0.01403192]\n",
      "[1.22777104e-05]\n",
      "[0.01401964]\n",
      "[6.1176387e-06]\n",
      "[0.01401352]\n",
      "[3.77662401e-06]\n",
      "[0.01400975]\n",
      "[2.74642648e-06]\n",
      "[0.014007]\n",
      "[2.21559497e-06]\n",
      "[0.01400478]\n",
      "[1.87744874e-06]\n",
      "[0.01400291]\n",
      "[1.62571003e-06]\n",
      "[0.01400128]\n",
      "[1.13788267e-06]\n",
      "[0.01400014]\n",
      "[0.01400337]\n",
      "[0.02800351]\n",
      "[0.01393289]\n",
      "[0.01407062]\n",
      "[4.73859135e-05]\n",
      "[0.01402323]\n",
      "[1.59362031e-05]\n",
      "[0.0140073]\n",
      "[6.41924507e-06]\n",
      "[0.01400088]\n",
      "[2.92959729e-06]\n",
      "[0.01399795]\n",
      "[1.57883583e-06]\n",
      "[0.01399637]\n",
      "[1.05239123e-06]\n",
      "[0.01399532]\n",
      "[8.15338483e-07]\n",
      "[0.0139945]\n",
      "[6.80014043e-07]\n",
      "[0.01399382]\n",
      "[5.85726528e-07]\n",
      "[0.01399324]\n",
      "[3.29683453e-07]\n",
      "[0.01399291]\n",
      "[0.00979912]\n",
      "[0.02379202]\n",
      "[0.0097345]\n",
      "[0.01405753]\n",
      "[4.2702246e-05]\n",
      "[0.01401482]\n",
      "[1.44465953e-05]\n",
      "[0.01400038]\n",
      "[5.50864701e-06]\n",
      "[0.01399487]\n",
      "[2.13970075e-06]\n",
      "[0.01399273]\n",
      "[9.02236848e-07]\n",
      "[0.01399183]\n",
      "[4.83920829e-07]\n",
      "[0.01399134]\n",
      "[3.36155191e-07]\n",
      "[0.01399101]\n",
      "[2.71975295e-07]\n",
      "[0.01399074]\n",
      "[2.12982925e-07]\n",
      "[0.01399052]\n",
      "[0.00221988]\n",
      "[0.01621041]\n",
      "[0.00212414]\n",
      "[0.01408626]\n",
      "[6.54128272e-05]\n",
      "[0.01402085]\n",
      "[1.91144477e-05]\n",
      "[0.01400174]\n",
      "[6.99000377e-06]\n",
      "[0.01399475]\n",
      "[2.69901074e-06]\n",
      "[0.01399205]\n",
      "[1.09003821e-06]\n",
      "[0.01399096]\n",
      "[5.1572946e-07]\n",
      "[0.01399044]\n",
      "[3.06815986e-07]\n",
      "[0.01399014]\n",
      "[2.18815363e-07]\n",
      "[0.01398992]\n",
      "[1.51260825e-07]\n",
      "[0.01398976]\n",
      "[0.00133935]\n",
      "[0.01532911]\n",
      "[0.00125382]\n",
      "[0.01407529]\n",
      "[5.72297808e-05]\n",
      "[0.01401806]\n",
      "[1.78277444e-05]\n",
      "[0.01400023]\n",
      "[6.68841933e-06]\n",
      "[0.01399354]\n",
      "[2.4836093e-06]\n",
      "[0.01399106]\n",
      "[9.07997494e-07]\n",
      "[0.01399015]\n",
      "[3.75466889e-07]\n",
      "[0.01398977]\n",
      "[2.00935557e-07]\n",
      "[0.01398957]\n",
      "[1.37894153e-07]\n",
      "[0.01398943]\n",
      "[9.41282743e-08]\n",
      "[0.01398934]\n",
      "[0.00158299]\n",
      "[0.01557233]\n",
      "[0.00148898]\n",
      "[0.01408334]\n",
      "[6.49835841e-05]\n",
      "[0.01401836]\n",
      "[1.82535241e-05]\n",
      "[0.01400011]\n",
      "[6.50871864e-06]\n",
      "[0.0139936]\n",
      "[2.47059127e-06]\n",
      "[0.01399113]\n",
      "[9.67906329e-07]\n",
      "[0.01399016]\n",
      "[4.31994617e-07]\n",
      "[0.01398973]\n",
      "[2.38029688e-07]\n",
      "[0.01398949]\n",
      "[1.58450527e-07]\n",
      "[0.01398933]\n",
      "[1.19640964e-07]\n",
      "[0.01398921]\n",
      "[8.07113321e-06]\n",
      "[0.01399728]\n",
      "[0.0002041]\n",
      "[0.01420138]\n",
      "[0.00016057]\n",
      "[0.01404081]\n",
      "[3.24658577e-05]\n",
      "[0.01400834]\n",
      "[1.17214658e-05]\n",
      "[0.01399662]\n",
      "[4.42685876e-06]\n",
      "[0.0139922]\n",
      "[1.72409474e-06]\n",
      "[0.01399047]\n",
      "[6.88985228e-07]\n",
      "[0.01398978]\n",
      "[3.17505167e-07]\n",
      "[0.01398946]\n",
      "[1.80710868e-07]\n",
      "[0.01398928]\n",
      "[1.23621882e-07]\n",
      "[0.01398916]\n",
      "[1.95569051e-07]\n",
      "[0.01398936]\n",
      "[0.0078624]\n",
      "[0.02185175]\n",
      "[0.00779452]\n",
      "[0.01405723]\n",
      "[4.34693882e-05]\n",
      "[0.01401377]\n",
      "[1.51760946e-05]\n",
      "[0.01399859]\n",
      "[5.69574946e-06]\n",
      "[0.01399289]\n",
      "[2.21035361e-06]\n",
      "[0.01399068]\n",
      "[8.58426751e-07]\n",
      "[0.01398982]\n",
      "[3.7009726e-07]\n",
      "[0.01398945]\n",
      "[1.95944127e-07]\n",
      "[0.01398926]\n",
      "[1.27497931e-07]\n",
      "[0.01398913]\n",
      "[3.32609106e-08]\n",
      "[0.0139891]\n",
      "[0.00709046]\n",
      "[0.02107956]\n",
      "[0.00701125]\n",
      "[0.01406831]\n",
      "[5.38889273e-05]\n",
      "[0.01401442]\n",
      "[1.58805626e-05]\n",
      "[0.01399854]\n",
      "[5.68610667e-06]\n",
      "[0.01399285]\n",
      "[2.15517591e-06]\n",
      "[0.0139907]\n",
      "[8.44903779e-07]\n",
      "[0.01398985]\n",
      "[3.79955043e-07]\n",
      "[0.01398947]\n",
      "[2.1025527e-07]\n",
      "[0.01398926]\n",
      "[1.39753769e-07]\n",
      "[0.01398912]\n",
      "[3.05736293e-07]\n",
      "[0.01398943]\n",
      "[0.00532077]\n",
      "[0.0193102]\n",
      "[0.00524938]\n",
      "[0.01406082]\n",
      "[4.62318149e-05]\n",
      "[0.01401459]\n",
      "[1.58016738e-05]\n",
      "[0.01399879]\n",
      "[5.7457133e-06]\n",
      "[0.01399304]\n",
      "[2.24069603e-06]\n",
      "[0.0139908]\n",
      "[9.01884558e-07]\n",
      "[0.0139899]\n",
      "[4.08568372e-07]\n",
      "[0.01398949]\n",
      "[2.24122212e-07]\n",
      "[0.01398927]\n",
      "[1.46871971e-07]\n",
      "[0.01398912]\n",
      "[5.46417094e-08]\n",
      "[0.01398906]\n",
      "[0.00590525]\n",
      "[0.01989432]\n",
      "[0.00582382]\n",
      "[0.01407049]\n",
      "[5.42785083e-05]\n",
      "[0.01401622]\n",
      "[1.70564928e-05]\n",
      "[0.01399916]\n",
      "[6.05675024e-06]\n",
      "[0.0139931]\n",
      "[2.3154637e-06]\n",
      "[0.01399079]\n",
      "[9.15782732e-07]\n",
      "[0.01398987]\n",
      "[4.08683515e-07]\n",
      "[0.01398946]\n",
      "[2.22435454e-07]\n",
      "[0.01398924]\n",
      "[1.45781453e-07]\n",
      "[0.01398909]\n",
      "[9.280436e-08]\n",
      "[0.013989]\n",
      "[0.00170596]\n",
      "[0.01569496]\n",
      "[0.00161172]\n",
      "[0.01408325]\n",
      "[6.50252206e-05]\n",
      "[0.01401822]\n",
      "[1.83171191e-05]\n",
      "[0.0139999]\n",
      "[6.55537932e-06]\n",
      "[0.01399335]\n",
      "[2.48834887e-06]\n",
      "[0.01399086]\n",
      "[9.70223809e-07]\n",
      "[0.01398989]\n",
      "[4.27867415e-07]\n",
      "[0.01398946]\n",
      "[2.31543498e-07]\n",
      "[0.01398923]\n",
      "[1.51541565e-07]\n",
      "[0.01398908]\n",
      "[6.37136273e-08]\n",
      "[0.01398902]\n",
      "[0.00779169]\n",
      "[0.02178071]\n",
      "[0.00772025]\n",
      "[0.01406046]\n",
      "[4.68713152e-05]\n",
      "[0.01401359]\n",
      "[1.53777098e-05]\n",
      "[0.01399821]\n",
      "[5.78100295e-06]\n",
      "[0.01399243]\n",
      "[2.14102368e-06]\n",
      "[0.01399029]\n",
      "[7.70705628e-07]\n",
      "[0.01398951]\n",
      "[3.07771976e-07]\n",
      "[0.01398921]\n",
      "[1.56978442e-07]\n",
      "[0.01398905]\n",
      "[1.04219332e-07]\n",
      "[0.01398895]\n",
      "[2.7907843e-08]\n",
      "[0.01398897]\n",
      "[0.01562271]\n",
      "[0.02961168]\n",
      "[0.01554118]\n",
      "[0.01407051]\n",
      "[5.28983324e-05]\n",
      "[0.01401761]\n",
      "[1.7838425e-05]\n",
      "[0.01399977]\n",
      "[6.44451428e-06]\n",
      "[0.01399332]\n",
      "[2.49195898e-06]\n",
      "[0.01399083]\n",
      "[9.89503772e-07]\n",
      "[0.01398984]\n",
      "[4.35695238e-07]\n",
      "[0.01398941]\n",
      "[2.32541086e-07]\n",
      "[0.01398917]\n",
      "[1.50391821e-07]\n",
      "[0.01398902]\n",
      "[9.09073844e-08]\n",
      "[0.01398893]\n",
      "[0.00242983]\n",
      "[0.01641877]\n",
      "[0.00233488]\n",
      "[0.01408388]\n",
      "[6.42216613e-05]\n",
      "[0.01401966]\n",
      "[1.91318798e-05]\n",
      "[0.01400053]\n",
      "[6.99366939e-06]\n",
      "[0.01399354]\n",
      "[2.65483708e-06]\n",
      "[0.01399088]\n",
      "[1.02414643e-06]\n",
      "[0.01398986]\n",
      "[4.44992322e-07]\n",
      "[0.01398941]\n",
      "[2.38611615e-07]\n",
      "[0.01398917]\n",
      "[1.56108565e-07]\n"
     ]
    }
   ],
   "source": [
    "def setupfig():\n",
    "    f = plt.figure()\n",
    "    f.set_size_inches(8,8)\n",
    "    ax = f.add_subplot(111)\n",
    "    plt.ion()\n",
    "    f.canvas.draw()\n",
    "    return (f,ax)\n",
    "\n",
    "def plotit(lams,nhiddens,erates,f,ax):\n",
    "    ax.clear()\n",
    "    for i in range(nhiddens.shape[0]):\n",
    "        ax.plot(lams,erates[:,i],'*-')\n",
    "    ax.set_yscale('log',subsy=[1,2,3,4,5,6,7,8,9])\n",
    "    ax.set_yticks([0.1,0.01])\n",
    "    ax.set_xscale('log')\n",
    "    f.canvas.draw()\n",
    "    ax.set_xlabel('lambda')\n",
    "    ax.set_ylabel('validation error rate')\n",
    "    ax.legend([(('# hidden units = '+str(x)) if x>0 else 'logistic regression') for x in nhiddens])\n",
    "    display.display(f)\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "nhiddens = np.array([0,4,20,100])\n",
    "lams = np.logspace(-3,2,10)\n",
    "erates = np.empty([lams.shape[0],nhiddens.shape[0]])\n",
    "erates[:,:] = np.nan\n",
    "\n",
    "(f,ax) = setupfig()\n",
    "\n",
    "    \n",
    "for ni, nhid in enumerate(nhiddens):\n",
    "    for li, lam in reversed(list(enumerate(lams))):\n",
    "        if ni==0:\n",
    "            w = learnlogreg(trainX,trainY,lam)\n",
    "            predy = predictlogreg(validX,w)[:,np.newaxis]\n",
    "        else:\n",
    "            (W1,W2) = trainneuralnet(trainX,trainY,nhid,lam)\n",
    "            predy = nneval(validX,W1,W2)\n",
    "        predy[predy<0.5] = 0.0\n",
    "        predy[predy>=0.5] = 1.0\n",
    "        validerr = (predy != validY).mean()\n",
    "        erates[li,ni] = validerr\n",
    "        \n",
    "        plotit(lams,nhiddens,erates,f,ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTERPRET the Plot [5 points]\n",
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "How do you interpret the plot above?  How and why does the plot differ by number of hidden units?  By $\\lambda$ value?  What parts of this plot agree with the material taught?  What parts do not?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answer Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS 171 Python",
   "language": "python",
   "name": "cs171-cpu-limitation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
